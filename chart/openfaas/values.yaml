functionNamespace: openfaas-fn  # Default namespace for functions

# Contact us via https://www.openfaas.com/support to purchase a license
openfaasPro: false

httpProbe: true               # Setting to true will use HTTP for readiness and liveness probe on the OpenFaaS core components
clusterRole: false            # Set to true for multiple namespaces, pro scaler and CPU/RAM metrics in OpenFaaS REST API
createCRDs: true              # Set to false if applying CRDs in another way

basic_auth: true              # Authentication for core components, no good reason to disable this
rbac: true                    # Kubernetes RBAC, no good reason to disable this
generateBasicAuth: false      # Set to false if applying credentials separately from the chart, otherwise set to true
securityContext: true

exposeServices: true
serviceType: NodePort        # serviceType for OpenFaaS gateway
async: true                  # No known reason to disable this, kept for legacy reasons 

# Set to true to use legacy / community-edition auto-scaling
# when openfaasPro is set to true to use the original 
# auto-scaling logic
# Then set proScaler.enabled=false
ceScaling: false

# create pod security policies for OpenFaaS control plane
# https://kubernetes.io/docs/concepts/policy/pod-security-policy/
psp: false

# image pull policy for openfaas components, can change to `IfNotPresent` in offline env
openfaasImagePullPolicy: "Always"

# openfaasPro components, which require openfaasPro=true
# clusterRole is also recommended for collecting CPU/RAM metrics for Pro add-ons

# OpenFaaS Pro
# Advanced auto-scaler for scaling functions on RPS, CPU and in-flight requests
# Includes: scale to zero
autoscaler:
  image: ghcr.io/openfaasltd/autoscaler:0.2.3
  replicas: 1
  enabled: false
  resources:
    requests:
      memory: "128Mi"
    limits:
      memory: "256Mi"
  # When disableHorizontalScaling is set to true, then the autoscaler will 
  # only scale to zero, without scaling replicas between the defined Min and Max
  # count for the function
  disableHorizontalScaling: false

# OpenFaaS Pro
# Original scale to zero feature
# set "enabled=false" when using proScaler
# since the proScaler handles scaling to zero now.
faasIdler:
  image: ghcr.io/openfaasltd/faas-idler:0.5.0
  replicas: 1
  enabled: false
  inactivityDuration: 3m                # If a function is inactive for 15 minutes, it may be scaled to zero
  reconcileInterval: 2m                 # The interval between each attempt to scale functions to zero
  readOnly: false                       # When set to true, no functions are scaled to zero
  writeDebug: false                     # Write additional debug information
  resources:
    requests:
      memory: "64Mi"

# OpenFaaS Pro
# OIDC plugin for authentication on the OpenFaaS REST API
oidcAuthPlugin:
  enabled: false
  verbose: false # debug setting
  provider: "" # Leave blank, or put "azure"
  insecureTLS: false
  scopes: "openid profile email"
  openidURL: "https://example.eu.auth0.com/.well-known/openid-configuration"
  audience: https://example.eu.auth0.com/api/v2/
  welcomePageURL: https://gateway.openfaas.example.com
  cookieDomain: ".openfaas.example.com"
  baseHost: "https://auth.openfaas.example.com"
  clientSecret: ""
  clientID: ""
  resources:
    requests:
      memory: "120Mi"
      cpu: "50m"
  replicas: 1
  image: ghcr.io/openfaasltd/openfaas-oidc-plugin:0.5.2
  securityContext: true

gateway:
  image: ghcr.io/openfaas/gateway:0.21.3
  readTimeout: "65s"
  writeTimeout: "65s"
  upstreamTimeout: "60s"  # Must be smaller than read/write_timeout
  replicas: 1
  scaleFromZero: true
  # change the port when creating multiple releases in the same baremetal cluster
  nodePort: 31112
  maxIdleConns: 1024
  maxIdleConnsPerHost: 1024
  directFunctions: false
  # Custom logs provider url. For example openfaas-loki would be
  # "http://ofloki-openfaas-loki.openfaas:9191/"
  logsProviderURL: ""
  resources:
    requests:
      memory: "120Mi"
      cpu: "50m"

basicAuthPlugin:
  image: ghcr.io/openfaas/basic-auth:0.21.3
  replicas: 1
  resources:
    requests:
      memory: "50Mi"
      cpu: "20m"

faasnetesPro:
  image: ghcr.io/openfaasltd/faas-netes:0.1.3

operatorPro:
  image: ghcr.io/openfaasltd/faas-netes:0.1.3

faasnetes:
  image: ghcr.io/openfaas/faas-netes:0.14.2
  readTimeout: "60s"
  writeTimeout: "60s"
  imagePullPolicy: "Always"    # Image pull policy for deployed functions
  httpProbe: true              # Setting to true will use HTTP for readiness and liveness probe on function pods
  setNonRootUser: false        # It's recommended to set this to "true", but test your images before committing to it
  readinessProbe:
    initialDelaySeconds: 2
    timeoutSeconds: 1           # Tuned-in to run checks early and quickly to support fast cold-start from zero replicas
    periodSeconds: 2            # Reduce to 1 for a faster cold-start, increase higher for lower-CPU usage
  livenessProbe:
    initialDelaySeconds: 2
    timeoutSeconds: 1
    periodSeconds: 2           # Reduce to 1 for a faster cold-start, increase higher for lower-CPU usage
  resources:
    requests:
      memory: "120Mi"
      cpu: "50m"

# replaces faas-netes with openfaas-operator
operator:
  image: ghcr.io/openfaas/faas-netes:0.14.2
  create: false
  # set this to false when creating multiple releases in the same cluster
  # must be true for the first one only
  createCRD: true
  resources:
    requests:
      memory: "120Mi"
      cpu: "50m"

# OpenFaaS Pro
# The values for queueWorkerPro are merged with those under
# the "queueWorker" section
#
# Enabled automatically when openfaasPro is set to true
queueWorkerPro:
  image: ghcr.io/openfaasltd/queue-worker:0.1.5
  maxRetryAttempts: "10"
  maxRetryWait: "120s"
  initialRetryWait: "10s"
  httpRetryCodes: "429,502,500,504,408"
  printRequestBody: false
  printResponseBody: false

queueWorker:
  image: ghcr.io/openfaas/queue-worker:0.12.2
  # Control HA of queue-worker
  replicas: 1
  # Control the concurrent invocations
  maxInflight: 1
  gatewayInvoke: true
  queueGroup: "faas"
  ackWait: "60s"
  resources:
    requests:
      memory: "120Mi"
      cpu: "50m"

# monitoring and auto-scaling components
# both components
prometheus:
  image: prom/prometheus:v2.11.0
  create: true
  resources:
    requests:
      memory: "512Mi"
  annotations: {}

alertmanager:
  image: prom/alertmanager:v0.18.0
  create: true
  resources:
    requests:
      memory: "25Mi"
    limits:
      memory: "50Mi"

# NATS is required for async
nats:
  channel: "faas-request"
  external:
    clusterName: ""
    enabled: false
    host: ""
    port: ""
  image: nats-streaming:0.22.0
  enableMonitoring: false
  metrics:
    # Should stay off by default because the exporter is not multi-arch (yet)
    enabled: false
    image: natsio/prometheus-nats-exporter:0.8.0
  resources:
    requests:
      memory: "120Mi"

# ingress configuration
ingress:
  enabled: false
  ## For k8s >= 1.18 you need to specify the pathType
  ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
  #pathType: ImplementationSpecific

  # Used to create Ingress record (should be used with exposeServices: false).
  hosts:
    - host: gateway.openfaas.local  # Replace with gateway.example.com if public-facing
      serviceName: gateway
      servicePort: 8080
      path: /
  annotations:
    kubernetes.io/ingress.class: nginx
  tls:
  # Secrets must be manually created in the namespace.

# ingressOperator (optional) â€“ component to have specific FQDN and TLS for Functions
# https://github.com/openfaas-incubator/ingress-operator
ingressOperator:
  image: ghcr.io/openfaas/ingress-operator:0.7.1
  replicas: 1
  create: false
  resources:
    requests:
      memory: "25Mi"

nodeSelector: {}

tolerations: []

affinity: {}

kubernetesDNSDomain: cluster.local

istio:
  mtls: false

gatewayExternal:
  annotations: {}

k8sVersionOverride: "" #  Allow kubeVersion to be overridden for the ingress creation
